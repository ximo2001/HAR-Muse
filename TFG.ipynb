{"cells":[{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","from sklearn.model_selection import LeaveOneOut, cross_val_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from tabulate import tabulate\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from scipy.fft import fft\n","import time\n","from pyentrp import entropy as ent\n","from sklearn.model_selection import GroupKFold\n","\n","# Function to load and preprocess data\n","def load_data(subject, repetition, experiment_type, minute):\n","    file_path = os.path.join(data_path, f'{subject}_{repetition}_{experiment_type}{minute}.csv')\n","\n","    # Read data, treating potential mixed types as strings, and coercing errors to NaN\n","    data = pd.read_csv(file_path, dtype=str, low_memory=False, on_bad_lines='skip')    \n","\n","    data = data.apply(pd.to_numeric, errors='coerce') \n","\n","    # Label assignment based on experiment type\n","    if experiment_type == 'm':\n","        data['label'] = 0\n","    elif experiment_type == 'l':\n","        data['label'] = 1\n","    elif experiment_type == 'c':\n","        data['label'] = 2\n","    elif experiment_type == 'e':\n","        data['label'] = 3\n","    else:\n","        raise ValueError(\"Invalid experiment_type.\")\n","\n","    return data\n","\n","# Determinar el número óptimo de características usando SelectKBest con validación cruzada\n","def optimal_k(features, labels):\n","    scores = []\n","    for k in range(1, min(20, features.shape[1]+1)):  # Asumimos un máximo de 20 características o menos si hay menos disponibles\n","        selector = SelectKBest(f_classif, k=k)\n","        selected_features = selector.fit_transform(features, labels)\n","        score = np.mean(cross_val_score(LinearSVC(dual=False), selected_features, labels, cv=5))\n","        scores.append((k, score))\n","    best_k = sorted(scores, key=lambda x: x[1], reverse=True)[0]\n","    print(f\"Optimal number of features: {best_k[0]} with cross-validation score: {best_k[1]:.2f}\")\n","    return best_k[0]\n","\n","# Function to reduce features using SelectKBest\n","def select_features(X_train, y_train, X_test, k=10):\n","    # Initialize and fit SelectKBest\n","    selector = SelectKBest(score_func=f_classif, k=k)\n","    selector.fit(X_train, y_train)\n","    \n","    # Transform both training and testing data\n","    X_train_reduced = selector.transform(X_train)\n","    X_test_reduced = selector.transform(X_test)\n","    \n","    return X_train_reduced, X_test_reduced\n","\n","# Function to help calculate specifity and sensibility\n","def calc_metrics(cm):\n","    # Sum all confusion matrix entries to get total number of instances\n","    total = cm.sum()\n","\n","    # Sum along the main diagonal to get all true positives\n","    TP = np.diag(cm)\n","\n","    # Calculate False Positives, False Negatives, and True Negatives\n","    FP = cm.sum(axis=0) - TP\n","    FN = cm.sum(axis=1) - TP\n","    TN = total - (FP + FN + TP)\n","\n","    # Calculate sensitivity (recall) and specificity for each class\n","    sensitivity = TP / (TP + FN)\n","    specificity = TN / (TN + FP)\n","\n","    # Calculate average sensitivity and specificity if needed\n","    avg_sensitivity = np.mean(sensitivity)\n","    avg_specificity = np.mean(specificity)\n","\n","    return avg_sensitivity, avg_specificity"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def calculate_total_band_energy(signal):\n","    \"\"\"Calculates the total energy of the frequency band of a signal using FFT.\"\"\"\n","    fft_values = np.fft.fft(signal)\n","    magnitude_squared = np.abs(fft_values) ** 2\n","    total_energy = np.sum(magnitude_squared) / len(signal)\n","    return total_energy\n","\n","def max_power(signal):\n","    \"\"\"Calculates the maximum power of a signal using FFT.\"\"\"\n","    fft_values = np.fft.fft(signal)\n","    power_spectrum = np.abs(fft_values) ** 2 / len(signal)\n","    return np.max(power_spectrum)\n","\n","def shannon_entropy(signal):\n","    \"\"\"Calculates Shannon entropy of a signal.\"\"\"\n","    value, counts = np.unique(signal, return_counts=True)\n","    probabilities = counts / counts.sum()\n","    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # To avoid log(0)\n","    return entropy\n","\n","def calculate_peak_to_peak(signal):\n","    \"\"\"Calculates the peak-to-peak value of a signal.\"\"\"\n","    peak_to_peak_value = signal.max() - signal.min()\n","    return peak_to_peak_value\n","\n","def differential_entropy(signal):\n","    variance = np.var(signal)\n","    if variance == 0:\n","        return 0  # Avoid log(0), return 0 for a signal with no variability\n","    return 0.5 * np.log(2 * np.pi * np.e * variance)\n","\n","\n","def sample_entropy(signal, m=2, r=None):\n","    try:\n","        std_signal = np.std(signal)\n","        if std_signal == 0:\n","            return 0  # Return a default or placeholder value that maintains consistency\n","        if r is None:\n","            r = 0.2 * std_signal\n","        entropy_value = ent.sample_entropy(signal, m, r)\n","        if len(entropy_value) > 0:\n","            return entropy_value[0]  # Ensure this returns a single value\n","        else:\n","            return 0  # Return a default value if entropy array is empty\n","    except Exception as e:\n","        print(f\"Error calculating sample entropy: {str(e)}\")\n","        return 0  # Return a default value in case of other exceptions\n","\n","\n","\n","def extract_features(data, num_windows):\n","    window_size = int((60*256) / num_windows)  # Adjusted to dataset specifics\n","    features, labels = [], []\n","    for i in range(num_windows):\n","        window_data = data.iloc[i*window_size : (i+1)*window_size]\n","        if len(window_data) < window_size:\n","            raise ValueError(\"NOT ENOUGH DATA FOR 1 MINUTE.\")\n","        \n","        # Drop label from the window data if present\n","        if 'label' in window_data.columns:\n","            labels.append(window_data['label'].iloc[0])\n","            feature_data = window_data.drop(columns='label')\n","        else:\n","            feature_data = window_data\n","\n","        # Calculate features for each channel\n","        means = feature_data.mean()\n","        max_powers = feature_data.apply(max_power)\n","        entropies = feature_data.apply(shannon_entropy)\n","        band_energies = feature_data.apply(calculate_total_band_energy)\n","        peak_to_peak_values = feature_data.apply(calculate_peak_to_peak)\n","        diff_entropies = feature_data.apply(differential_entropy)\n","        #sample_entropies = feature_data.apply(sample_entropy)\n","        #sample_entropies_array = sample_entropies.values.flatten() if sample_entropies.ndim > 1 else sample_entropies.values\n","\n","        # Combine all features into a single array\n","        combined_features = np.concatenate([\n","            means.values,\n","            max_powers.values,\n","            entropies.values,\n","            band_energies.values,\n","            #sample_entropies_array,\n","            diff_entropies.values,\n","            peak_to_peak_values.values            \n","        ])\n","\n","        # Append combined features and labels\n","        features.append(combined_features)\n","\n","    features, labels = np.array(features), np.array(labels)\n","    return features, labels\n","\n","def generate_feature_names(data):\n","    feature_names = []\n","    # Assuming 'data' is a DataFrame with the same structure as your actual feature data\n","    sample_data = data.iloc[:1]  # Take just one row to minimize processing\n","    if 'label' in sample_data.columns:\n","        sample_data = sample_data.drop(columns='label')\n","\n","    for column in sample_data.columns:\n","        feature_names.extend([\n","            f\"mean_{column}\",\n","            f\"max_power_{column}\",\n","            f\"entropy_{column}\",\n","            f\"band_energy_{column}\",\n","            #f\"sample_entropy_{column}\", \n","            f\"differential_entropy_{column}\",\n","            f\"peak_to_peak_{column}\"\n","        ])\n","    return feature_names\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def save_features_labels(features, labels, subject_ids, num_windows, feature_names, folder_path):\n","    \"\"\"\n","    Saves features, labels, and subject IDs to a CSV file, ensuring not to overwrite existing files.\n","    :param features: numpy array of features.\n","    :param labels: numpy array of labels.\n","    :param subject_ids: numpy array of subject identifiers.\n","    :param num_windows: number of windows, used for naming the file.\n","    :param folder_path: directory path where the files will be saved.\n","    \"\"\"\n","    # Create the folder if it does not exist\n","    os.makedirs(folder_path, exist_ok=True)\n","    \n","    # Prepare the data for saving\n","    data = np.column_stack((subject_ids, features, labels))\n","    df = pd.DataFrame(data)\n","    df.columns = [\"subject_id\"] + feature_names + [\"label\"]\n","    \n","    # Generate the base file name\n","    base_file_name = os.path.join(folder_path, f\"features_{num_windows}\")\n","    extension = \".csv\"\n","    file_name = base_file_name + extension\n","    counter = 1\n","\n","    # Increment the file name if it already exists\n","    while os.path.exists(file_name):\n","        file_name = f\"{base_file_name}({counter}){extension}\"\n","        counter += 1\n","\n","    # Save the DataFrame to a CSV file\n","    df.to_csv(file_name, index=False)\n","    print(f\"File saved: {file_name}\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File saved: /home/ximo/Escritorio/ProyectoTFG/features/features_1.csv\n","File saved: /home/ximo/Escritorio/ProyectoTFG/features/features_2.csv\n"]}],"source":["# RAPIDO Y MODULADO TODAVIA SE PODRÍA HACER LA EXTRACCIÓN DE CARACTERÍSTICAS A PARTE\n","# Path to data and other constants\n","data_path = '/home/ximo/Escritorio/ProyectoTFG/MusePreprocessed'\n","subjects = range(1,31)\n","repetitions = ['1', '2']\n","minutes = ['1', '2', '3']\n","experiment_types = ['m', 'l', 'c', 'e']\n","num_windows_options = [1,2]\n","folder_name = \"/home/ximo/Escritorio/ProyectoTFG/features\"\n","\n","# Setting up leave-one-out cross-validation\n","loo = LeaveOneOut()\n","\n","for num_windows in num_windows_options:\n","\n","    # Preload all data and extract features once\n","    all_data = {}\n","    all_features = []\n","    all_labels = []\n","    all_subject_ids = []  \n","\n","    # Load a small sample data to generate feature names\n","    sample_data = load_data(subjects[0], repetitions[0], experiment_types[0], minutes[0])\n","    feature_names = generate_feature_names(sample_data)\n","\n","\n","    for subject in subjects:\n","        subject_data = []\n","        for repetition in repetitions:\n","            for exp_type in experiment_types:\n","                for minute in minutes:\n","                    data = load_data(subject, repetition, exp_type, minute)\n","                    features, labels = extract_features(data, num_windows)\n","                    subject_data.append((features, labels))\n","                    all_features.extend(features)\n","                    all_labels.extend(labels)\n","                    all_subject_ids.extend([subject] * len(features))\n","        all_data[subject] = subject_data\n","\n","    all_features = np.array(all_features)\n","    all_labels = np.array(all_labels)\n","    all_subject_ids = np.array(all_subject_ids)  # Convert list of subject IDs to an array\n","\n","    #Guardamos las features en un ficheros\n","    save_features_labels(all_features, all_labels, all_subject_ids, num_windows, feature_names, folder_name)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal number of features: 8 with cross-validation score: 0.41\n"]},{"name":"stderr","output_type":"stream","text":["/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Optimal number of features: 17 with cross-validation score: 0.40\n"]},{"name":"stderr","output_type":"stream","text":["/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/home/ximo/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["def process_and_evaluate(file_path):\n","    df = pd.read_csv(file_path)\n","    features = df.drop(columns=['subject_id', 'label'])\n","    labels = df['label']\n","    subjects = df['subject_id']\n","\n","    # Select optimal k features\n","    k = optimal_k(features, labels)  # Implement this function based on your criteria\n","    selector = SelectKBest(f_classif, k=k)\n","    selected_features = selector.fit_transform(features, labels)\n","\n","    # Setup cross-validation\n","    gkf = GroupKFold(n_splits=len(np.unique(subjects)))  # Number of unique subjects\n","\n","    # Model initialization\n","    models = {\n","        \"SVM\": LinearSVC(dual=True, max_iter=100),\n","        \"Random Forest\": RandomForestClassifier(n_estimators=100),\n","        \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n","        \"KNN\": KNeighborsClassifier(n_neighbors=4)\n","    }\n","\n","    results_text = \"\"\n","\n","    # Cross-validation by subject\n","    for train_idx, test_idx in gkf.split(selected_features, labels, groups=subjects):\n","        X_train, X_test = selected_features[train_idx], selected_features[test_idx]\n","        y_train, y_test = labels.iloc[train_idx], labels.iloc[test_idx]\n","\n","        # Store results for each model\n","        model_results = []\n","        for name, model in models.items():\n","            model.fit(X_train, y_train)\n","            predictions = model.predict(X_test)\n","            accuracy = accuracy_score(y_test, predictions)\n","            cm = confusion_matrix(y_test, predictions)\n","            sensitivity, specificity = calc_metrics(cm)\n","\n","            model_results.append([name, accuracy, specificity, sensitivity])\n","\n","        results_text += f\"Results for file: {os.path.basename(file_path)}\\n\"\n","        results_text += tabulate(model_results, headers=[\"Model\", \"Accuracy\", \"Specificity\", \"Sensitivity\"]) + \"\\n\\n\"\n","\n","    return results_text\n","\n","def main(folder_path, output_path):\n","    results_text = \"\"\n","    for file_name in os.listdir(folder_path):\n","        if file_name.startswith(\"features_\") and file_name.endswith(\".csv\"):\n","            file_path = os.path.join(folder_path, file_name)\n","            results_text += process_and_evaluate(file_path)\n","\n","    # Write results to the output file\n","    with open(output_path, 'w') as f:\n","        f.write(results_text)\n","\n","# Paths setup\n","folder_path = '/home/ximo/Escritorio/ProyectoTFG/features'\n","output_path = '/home/ximo/Escritorio/ProyectoTFG/results.txt'\n","main(folder_path, output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # Select optimal k features\n","    k = optimal_k(all_features, all_labels)\n","    selector = SelectKBest(f_classif, k=k)\n","    all_features = selector.fit_transform(all_features, all_labels)\n","\n","    # Aplicar la selección de características a los datos de cada sujeto\n","    for subject, data in all_data.items():\n","        new_data = []\n","        for features, labels in data:\n","            selected_features = selector.transform(features)\n","            new_data.append((selected_features, labels))\n","        all_data[subject] = new_data\n","            \n","    svm_accuracies, rf_accuracies, xgb_accuracies, knn_accuracies = [], [], [], []\n","    svm_sensitivities, rf_sensitivities, xgb_sensitivities, knn_sensitivities = [], [], [], []\n","    svm_specificities, rf_specificities, xgb_specificities, knn_specificities = [], [], [], []\n","\n","    loop_start_time = time.time()\n","\n","    for train_index, test_index in loo.split(subjects):\n","        \n","        train_subjects = [subjects[i] for i in train_index]\n","        test_subject = subjects[test_index[0]]\n","\n","        # Combine training data\n","        X_train, y_train = [], []\n","        for subject in train_subjects:\n","            for feature_set, label_set in all_data[subject]:\n","                X_train.extend(feature_set)\n","                y_train.extend(label_set)\n","\n","        X_train = np.array(X_train)\n","        y_train = np.array(y_train)\n","\n","        # Use test data\n","        X_test, y_test = [], []\n","        for feature_set, label_set in all_data[test_subject]:\n","            X_test.extend(feature_set)\n","            y_test.extend(label_set)\n","\n","        X_test = np.array(X_test)\n","        y_test = np.array(y_test)\n","        \n","        svm_model = LinearSVC(dual=True, max_iter=100, verbose=False)\n","        svm_model.fit(X_train, y_train)\n","        svm_predictions = svm_model.predict(X_test)\n","        svm_accuracy = accuracy_score(y_test, svm_predictions)\n","        svm_accuracies.append(svm_accuracy)\n","\n","        print(\"SVM\\n\")\n","        \n","        # Train and Test Random Forest model\n","        rf_model = RandomForestClassifier(n_estimators=100)\n","        rf_model.fit(X_train, y_train)\n","        rf_predictions = rf_model.predict(X_test)\n","        rf_accuracy = accuracy_score(y_test, rf_predictions)\n","        rf_accuracies.append(rf_accuracy)\n","\n","        # Train and Test XGBoost model\n","        xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n","        xgb_model.fit(X_train, y_train)\n","        xgb_predictions = xgb_model.predict(X_test)\n","        xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n","        xgb_accuracies.append(xgb_accuracy)\n","\n","        # Train and Test KNN model\n","        knn_model = KNeighborsClassifier(n_neighbors=4)\n","        knn_model.fit(X_train, y_train)\n","        knn_predictions = knn_model.predict(X_test)\n","        knn_accuracy = accuracy_score(y_test, knn_predictions)\n","        knn_accuracies.append(knn_accuracy)\n","\n","        # We calculate the confusion Matrix\n","        cm_svm = confusion_matrix(y_test, svm_predictions)\n","        cm_rf = confusion_matrix(y_test, rf_predictions)\n","        cm_xgb = confusion_matrix(y_test, xgb_predictions)\n","        cm_knn = confusion_matrix(y_test, knn_predictions)\n","\n","        # After calculating the confusion matrix for each model we save specifity and sensibility\n","        sens_svm, spec_svm = calc_metrics(cm_svm)\n","        svm_sensitivities.append(sens_svm)\n","        svm_specificities.append(spec_svm)\n","\n","        sens_rf, spec_rf = calc_metrics(cm_rf)\n","        rf_sensitivities.append(sens_rf)\n","        rf_specificities.append(spec_rf)\n","\n","        sens_xgb, spec_xgb = calc_metrics(cm_xgb)\n","        xgb_sensitivities.append(sens_xgb)\n","        xgb_specificities.append(spec_xgb)\n","\n","        sens_knn, spec_knn = calc_metrics(cm_knn)\n","        knn_sensitivities.append(sens_knn)\n","        knn_specificities.append(spec_knn)\n","\n","        \"\"\"\n","        # Print results after each iteration\n","        print(f\"Results for test subject {test_subject}:\")\n","        print(f\"SVM - Accuracy: {svm_accuracy:.2f}%, Sensitivity: {sens_svm:.2f}%, Specificity: {spec_svm:.2f}%\")\n","        print(f\"Random Forest - Accuracy: {rf_accuracy:.2f}%, Sensitivity: {sens_rf:.2f}%, Specificity: {spec_rf:.2f}%\")\n","        print(f\"XGBoost - Accuracy: {xgb_accuracy:.2f}%, Sensitivity: {sens_xgb:.2f}%, Specificity: {spec_xgb:.2f}%\")\n","        print(f\"KNN - Accuracy: {knn_accuracy:.2f}%, Sensitivity: {sens_knn:.2f}%, Specificity: {spec_knn:.2f}%\")\n","        print(\"\\n\")\n","        \"\"\"\n","        \n","        #loop_end_time = time.time()\n","        #elapsed_time = loop_end_time - loop_start_time\n","        #print(f\"Elapsed time {elapsed_time:.2f} seconds\")\n","\n","\n","    seconds = 60/int(num_windows)\n","    print(f\"Done with {num_windows} windows of {seconds} seconds:\")\n","    results = [\n","        [\"SVM\", f\"{np.mean(svm_accuracies):.2f}%\", f\"{np.mean(svm_specificities):.2f}%\", f\"{np.mean(svm_sensitivities):.2f}%\"],\n","        [\"Random Forest\", f\"{np.mean(rf_accuracies):.2f}%\", f\"{np.mean(rf_specificities):.2f}%\", f\"{np.mean(rf_sensitivities):.2f}%\"],\n","        [\"XGBoost\", f\"{np.mean(xgb_accuracies):.2f}%\", f\"{np.mean(xgb_specificities):.2f}%\", f\"{np.mean(xgb_sensitivities):.2f}%\"],\n","        [\"KNN\", f\"{np.mean(knn_accuracies):.2f}%\", f\"{np.mean(knn_specificities):.2f}%\", f\"{np.mean(knn_sensitivities):.2f}%\"]\n","    ]\n","\n","    print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Specificity\", \"Sensitivity\"]))\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cargo los datos. \n","\n","Selecciono las features\n","Salgo SVM. \n","\n","Salgo de todos los modelos. \n","\n","Results for test subject 1:\n","SVM - Accuracy: 0.25%, Sensitivity: 0.25%, Specificity: 0.75%\n","Random Forest - Accuracy: 0.42%, Sensitivity: 0.42%, Specificity: 0.81%\n","XGBoost - Accuracy: 0.42%, Sensitivity: 0.42%, Specificity: 0.81%\n","KNN - Accuracy: 0.58%, Sensitivity: 0.58%, Specificity: 0.86%\n","\n","\n","Elapsed time 205.53 seconds\n","Cargo los datos. \n","\n","Selecciono las features\n","Salgo SVM. \n","\n","Salgo de todos los modelos. \n","\n","Results for test subject 2:\n","SVM - Accuracy: 0.21%, Sensitivity: 0.21%, Specificity: 0.74%\n","Random Forest - Accuracy: 0.25%, Sensitivity: 0.25%, Specificity: 0.75%\n","XGBoost - Accuracy: 0.25%, Sensitivity: 0.25%, Specificity: 0.75%\n","KNN - Accuracy: 0.25%, Sensitivity: 0.25%, Specificity: 0.75%\n","\n","\n","Elapsed time 419.32 seconds\n"]},{"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp_type \u001b[38;5;129;01min\u001b[39;00m experiment_types:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m minute \u001b[38;5;129;01min\u001b[39;00m minutes:\n\u001b[0;32m---> 30\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         features, labels \u001b[38;5;241m=\u001b[39m extract_features(data, num_windows)\n\u001b[1;32m     32\u001b[0m         X_train\u001b[38;5;241m.\u001b[39mextend(features)\n","Cell \u001b[0;32mIn[34], line 22\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(subject, repetition, experiment_type, minute)\u001b[0m\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepetition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mminute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Read data, treating potential mixed types as strings, and coercing errors to NaN\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Label assignment based on experiment type\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n","File \u001b[0;32mparsers.pyx:825\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:913\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}],"source":["# MUY LENTO NO MODULADO\n","# Path to data and other constants\n","data_path = '/home/ximo/Escritorio/ProyectoTFG/MusePreprocessed'\n","subjects = range(1,31)\n","repetitions = ['1', '2']\n","minutes = ['1', '2', '3']\n","experiment_types = ['m', 'l', 'c', 'e']\n","num_windows_options = [1]\n","\n","# Setting up leave-one-out cross-validation\n","loo = LeaveOneOut()\n","\n","for num_windows in num_windows_options:\n","    svm_accuracies, rf_accuracies, xgb_accuracies, knn_accuracies = [], [], [], []\n","    svm_sensitivities, rf_sensitivities, xgb_sensitivities, knn_sensitivities = [], [], [], []\n","    svm_specificities, rf_specificities, xgb_specificities, knn_specificities = [], [], [], []\n","\n","    loop_start_time = time.time()\n","\n","    for train_index, test_index in loo.split(subjects):\n","        \n","        train_subjects = [subjects[i] for i in train_index]\n","        test_subject = subjects[test_index[0]]\n","\n","        # Load and preprocess training data\n","        X_train, y_train = [], []\n","        for subject in train_subjects:\n","            for repetition in repetitions:\n","                for exp_type in experiment_types:\n","                    for minute in minutes:\n","                        data = load_data(subject, repetition, exp_type, minute)\n","                        features, labels = extract_features(data, num_windows)\n","                        X_train.extend(features)\n","                        y_train.extend(labels)\n","\n","        X_train = np.array(X_train)\n","        y_train = np.array(y_train)\n","\n","        # Load and preprocess test data\n","        X_test, y_test = [], []\n","        for repetition in repetitions:\n","            for exp_type in experiment_types:\n","                for minute in minutes:\n","                    data = load_data(test_subject, repetition, exp_type, minute)\n","                    features, labels = extract_features(data, num_windows)\n","                    X_test.extend(features)\n","                    y_test.extend(labels)\n","\n","        X_test = np.array(X_test)\n","        y_test = np.array(y_test)\n","\n","        print(\"Cargo los datos. \\n\")\n","\n","        # Reduce features using SelectKBest\n","        X_train, X_test = select_features(X_train, y_train, X_test, k=5)\n","\n","        print(\"Selecciono las features\")\n","\n","        # Train and Test SVM model\n","        svm_model = LinearSVC(dual=True, verbose=False)\n","        svm_model.fit(X_train, y_train)\n","        svm_predictions = svm_model.predict(X_test)\n","        svm_accuracy = accuracy_score(y_test, svm_predictions)\n","        svm_accuracies.append(svm_accuracy)\n","\n","        print(\"Salgo SVM. \\n\")\n","        \n","        # Train and Test Random Forest model\n","        rf_model = RandomForestClassifier(n_estimators=100)\n","        rf_model.fit(X_train, y_train)\n","        rf_predictions = rf_model.predict(X_test)\n","        rf_accuracy = accuracy_score(y_test, rf_predictions)\n","        rf_accuracies.append(rf_accuracy)\n","\n","        # Train and Test XGBoost model\n","        xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n","        xgb_model.fit(X_train, y_train)\n","        xgb_predictions = xgb_model.predict(X_test)\n","        xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n","        xgb_accuracies.append(xgb_accuracy)\n","\n","        # Train and Test KNN model\n","        knn_model = KNeighborsClassifier(n_neighbors=4)\n","        knn_model.fit(X_train, y_train)\n","        knn_predictions = knn_model.predict(X_test)\n","        knn_accuracy = accuracy_score(y_test, knn_predictions)\n","        knn_accuracies.append(knn_accuracy)\n","\n","        print(\"Salgo de todos los modelos. \\n\")\n","\n","        # We calculate the confusion Matrix\n","        cm_svm = confusion_matrix(y_test, svm_predictions)\n","        cm_rf = confusion_matrix(y_test, rf_predictions)\n","        cm_xgb = confusion_matrix(y_test, xgb_predictions)\n","        cm_knn = confusion_matrix(y_test, knn_predictions)\n","\n","        # After calculating the confusion matrix for each model we save specifity and sensibility\n","        sens_svm, spec_svm = calc_metrics(cm_svm)\n","        svm_sensitivities.append(sens_svm)\n","        svm_specificities.append(spec_svm)\n","\n","        sens_rf, spec_rf = calc_metrics(cm_rf)\n","        rf_sensitivities.append(sens_rf)\n","        rf_specificities.append(spec_rf)\n","\n","        sens_xgb, spec_xgb = calc_metrics(cm_xgb)\n","        xgb_sensitivities.append(sens_xgb)\n","        xgb_specificities.append(spec_xgb)\n","\n","        sens_knn, spec_knn = calc_metrics(cm_knn)\n","        knn_sensitivities.append(sens_knn)\n","        knn_specificities.append(spec_knn)\n","\n","        # Print results after each iteration\n","        print(f\"Results for test subject {test_subject}:\")\n","        print(f\"SVM - Accuracy: {svm_accuracy:.2f}%, Sensitivity: {sens_svm:.2f}%, Specificity: {spec_svm:.2f}%\")\n","        print(f\"Random Forest - Accuracy: {rf_accuracy:.2f}%, Sensitivity: {sens_rf:.2f}%, Specificity: {spec_rf:.2f}%\")\n","        print(f\"XGBoost - Accuracy: {xgb_accuracy:.2f}%, Sensitivity: {sens_xgb:.2f}%, Specificity: {spec_xgb:.2f}%\")\n","        print(f\"KNN - Accuracy: {knn_accuracy:.2f}%, Sensitivity: {sens_knn:.2f}%, Specificity: {spec_knn:.2f}%\")\n","        print(\"\\n\")\n","\n","        loop_end_time = time.time()\n","        elapsed_time = loop_end_time - loop_start_time\n","        print(f\"Elapsed time {elapsed_time:.2f} seconds\")\n","\n","\n","    seconds = 60/int(num_windows)\n","    print(f\"Done with {num_windows} windows of {seconds} seconds:\")\n","    results = [\n","        [\"SVM\", f\"{np.mean(svm_accuracies):.2f}%\", f\"{np.mean(svm_specificities):.2f}%\", f\"{np.mean(svm_sensitivities):.2f}%\"],\n","        [\"Random Forest\", f\"{np.mean(rf_accuracies):.2f}%\", f\"{np.mean(rf_specificities):.2f}%\", f\"{np.mean(rf_sensitivities):.2f}%\"],\n","        [\"XGBoost\", f\"{np.mean(xgb_accuracies):.2f}%\", f\"{np.mean(xgb_specificities):.2f}%\", f\"{np.mean(xgb_sensitivities):.2f}%\"],\n","        [\"KNN\", f\"{np.mean(knn_accuracies):.2f}%\", f\"{np.mean(knn_specificities):.2f}%\", f\"{np.mean(knn_sensitivities):.2f}%\"]\n","    ]\n","\n","    print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Specificity\", \"Sensitivity\"]))\n","    print(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def extract_and_save_features(subjects, num_windows, data_path):\n","    all_features = []\n","    all_labels = []\n","    for subject in subjects:\n","        for repetition in repetitions:\n","            for exp_type in experiment_types:\n","                for minute in minutes:\n","                    data = load_data(subject, repetition, exp_type, minute)\n","                    features, labels = extract_features(data, num_windows)\n","                    all_features.append(features)\n","                    all_labels.append(labels)\n","    all_features = np.array(all_features)\n","    all_labels = np.array(all_labels)\n","    filename = f\"{data_path}/features_labels_{num_windows}w.npz\"\n","    np.savez(filename, features=all_features, labels=all_labels)\n","    print(f\"Features and labels saved to {filename}\")\n","\n","# Ejemplo de uso:\n","data_path = '/home/ximo/Escritorio/ProyectoTFG/MusePreprocessed'\n","subjects = range(1, 31)\n","num_windows_options = [1]\n","for num_windows in num_windows_options:\n","    extract_and_save_features(subjects, num_windows, data_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2810/600204107.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexperiment_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_windows_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calculate global minimum and maximum values for normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmin_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_global_min_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcluded_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Setting up leave-one-out cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mloo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_2810/2328384033.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data_path, subjects, repetitions, experiment_types, minutes, excluded_columns)\u001b[0m\n\u001b[1;32m     41\u001b[0m                             \u001b[0mmax_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                             \u001b[0mmin_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                             \u001b[0mmax_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error processing file {file_path}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# After finishing all files:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10030\u001b[0m             \u001b[0mby_row\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10031\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10032\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10033\u001b[0m         )\n\u001b[0;32m> 10034\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;31m# raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mseries_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0mcol_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m             \u001b[0;31m# this is a cached value, mark it so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3807\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_as_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, values, loc)\u001b[0m\n\u001b[1;32m   4388\u001b[0m         \u001b[0;31m# Lookup in columns so that if e.g. a str datetime was passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0;31m#  we attach the Timestamp object as the name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m         \u001b[0;31m# We get index=self.index bc values is a SingleDataManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4392\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4393\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mgr, axes)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# caller is responsible for setting real name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Path to data and other constants\n","data_path = '/home/ximo/Escritorio/ProyectoTFG/Muse'\n","subjects = range(1,31)\n","repetitions = ['1', '2']\n","minutes = ['1', '2', '3']\n","experiment_types = ['m', 'l', 'c', 'e']\n","num_windows_options = [1]\n","\n","# Calculate global minimum and maximum values for normalization\n","min_vals, max_vals = compute_global_min_max(data_path, subjects, repetitions, experiment_types, minutes, excluded_columns)\n","\n","# Setting up leave-one-out cross-validation\n","loo = LeaveOneOut()\n","\n","for num_windows in num_windows_options:\n","    svm_accuracies, rf_accuracies, xgb_accuracies, knn_accuracies = [], [], [], []\n","    svm_sensitivities, rf_sensitivities, xgb_sensitivities, knn_sensitivities = [], [], [], []\n","    svm_specificities, rf_specificities, xgb_specificities, knn_specificities = [], [], [], []\n","\n","    loop_start_time = time.time()\n","\n","    for train_index, test_index in loo.split(subjects):\n","        train_subjects = [subjects[i] for i in train_index]\n","        test_subject = subjects[test_index[0]]\n","\n","        # Load and preprocess training data\n","        X_train, y_train = [], []\n","        for subject in train_subjects:\n","            for repetition in repetitions:\n","                for exp_type in experiment_types:\n","                    for minute in minutes:\n","                        data = load_data(subject, repetition, exp_type, minute, min_vals, max_vals)\n","                        features, labels = extract_features(data, num_windows)\n","                        X_train.extend(features)\n","                        y_train.extend(labels)\n","\n","        X_train = np.array(X_train)\n","        y_train = np.array(y_train)\n","\n","        # Load and preprocess test data\n","        X_test, y_test = [], []\n","        for repetition in repetitions:\n","            for exp_type in experiment_types:\n","                for minute in minutes:\n","                    data = load_data(test_subject, repetition, exp_type, minute, min_vals, max_vals)\n","                    features, labels = extract_features(data, num_windows)\n","                    X_test.extend(features)\n","                    y_test.extend(labels)\n","\n","        X_test = np.array(X_test)\n","        y_test = np.array(y_test)\n","\n","        # Reduce features using SelectKBest\n","        X_train, X_test = select_features(X_train, y_train, X_test, k=25)\n","\n","        # Train and Test SVM model\n","        svm_model = SVC(kernel='linear', verbose=False)\n","        svm_model.fit(X_train, y_train)\n","        svm_predictions = svm_model.predict(X_test)\n","        svm_accuracy = accuracy_score(y_test, svm_predictions)\n","        svm_accuracies.append(svm_accuracy)\n","        \n","        # Train and Test Random Forest model\n","        rf_model = RandomForestClassifier(n_estimators=100)\n","        rf_model.fit(X_train, y_train)\n","        rf_predictions = rf_model.predict(X_test)\n","        rf_accuracy = accuracy_score(y_test, rf_predictions)\n","        rf_accuracies.append(rf_accuracy)\n","\n","        # Train and Test XGBoost model\n","        xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n","        xgb_model.fit(X_train, y_train)\n","        xgb_predictions = xgb_model.predict(X_test)\n","        xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n","        xgb_accuracies.append(xgb_accuracy)\n","\n","        # Train and Test KNN model\n","        knn_model = KNeighborsClassifier(n_neighbors=4)\n","        knn_model.fit(X_train, y_train)\n","        knn_predictions = knn_model.predict(X_test)\n","        knn_accuracy = accuracy_score(y_test, knn_predictions)\n","        knn_accuracies.append(knn_accuracy)\n","\n","        # We calculate the confusion Matrix\n","        cm_svm = confusion_matrix(y_test, svm_predictions)\n","        cm_rf = confusion_matrix(y_test, rf_predictions)\n","        cm_xgb = confusion_matrix(y_test, xgb_predictions)\n","        cm_knn = confusion_matrix(y_test, knn_predictions)\n","\n","        # After calculating the confusion matrix for each model we save specifity and sensibility\n","        sens_svm, spec_svm = calc_metrics(cm_svm)\n","        svm_sensitivities.append(sens_svm)\n","        svm_specificities.append(spec_svm)\n","\n","        sens_rf, spec_rf = calc_metrics(cm_rf)\n","        rf_sensitivities.append(sens_rf)\n","        rf_specificities.append(spec_rf)\n","\n","        sens_xgb, spec_xgb = calc_metrics(cm_xgb)\n","        xgb_sensitivities.append(sens_xgb)\n","        xgb_specificities.append(spec_xgb)\n","\n","        sens_knn, spec_knn = calc_metrics(cm_knn)\n","        knn_sensitivities.append(sens_knn)\n","        knn_specificities.append(spec_knn)\n","\n","        # Print results after each iteration\n","        print(f\"Results for test subject {test_subject}:\")\n","        print(f\"SVM - Accuracy: {svm_accuracy:.2f}%, Sensitivity: {sens_svm:.2f}%, Specificity: {spec_svm:.2f}%\")\n","        print(f\"Random Forest - Accuracy: {rf_accuracy:.2f}%, Sensitivity: {sens_rf:.2f}%, Specificity: {spec_rf:.2f}%\")\n","        print(f\"XGBoost - Accuracy: {xgb_accuracy:.2f}%, Sensitivity: {sens_xgb:.2f}%, Specificity: {spec_xgb:.2f}%\")\n","        print(f\"KNN - Accuracy: {knn_accuracy:.2f}%, Sensitivity: {sens_knn:.2f}%, Specificity: {spec_knn:.2f}%\")\n","        print(\"\\n\")\n","\n","        loop_end_time = time.time()\n","        elapsed_time = loop_end_time - loop_start_time\n","        print(f\"Elapsed time {elapsed_time:.2f} seconds\")\n","        print(\"\\n\")\n","        print(\"\\n\")\n","\n","\n","\n","    seconds = 60/int(num_windows)\n","    print(f\"Done with {num_windows} windows of {seconds} seconds:\")\n","    results = [\n","        [\"SVM\", f\"{np.mean(svm_accuracies):.2f}%\", f\"{np.mean(svm_specificities):.2f}%\", f\"{np.mean(svm_sensitivities):.2f}%\"],\n","        [\"Random Forest\", f\"{np.mean(rf_accuracies):.2f}%\", f\"{np.mean(rf_specificities):.2f}%\", f\"{np.mean(rf_sensitivities):.2f}%\"],\n","        [\"XGBoost\", f\"{np.mean(xgb_accuracies):.2f}%\", f\"{np.mean(xgb_specificities):.2f}%\", f\"{np.mean(xgb_sensitivities):.2f}%\"],\n","        [\"KNN\", f\"{np.mean(knn_accuracies):.2f}%\", f\"{np.mean(knn_specificities):.2f}%\", f\"{np.mean(knn_sensitivities):.2f}%\"]\n","    ]\n","\n","    print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Specificity\", \"Sensitivity\"]))\n","    print(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def extract_features(data, num_windows):\n","    window_size = int((60*256) / num_windows)  # Adjusted to dataset specifics\n","    features, labels = [], []\n","    for i in range(num_windows):\n","        window_data = data.iloc[i*window_size : (i+1)*window_size]\n","        if len(window_data) < window_size:\n","            raise ValueError(\"NOT ENOUGH DATA FOR 1 MINUTE.\")\n","        \n","        # Drop label from the window data if present\n","        if 'label' in window_data.columns:\n","            labels.append(window_data['label'].iloc[0])\n","            feature_data = window_data.drop(columns='label')\n","        else:\n","            feature_data = window_data\n","\n","        # Calculate features for each channel\n","        means = feature_data.mean()\n","\n","        # Combine all features into a single array\n","        combined_features = np.concatenate([\n","            means.values,\n","        ])\n","\n","        # Append combined features and labels\n","        features.append(combined_features)\n","\n","    features, labels = np.array(features), np.array(labels)\n","    return features, labels"]},{"cell_type":"markdown","metadata":{},"source":["3 FEATURES BIEN:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def calculate_total_band_energy(signal):\n","    \"\"\"Calculates the total energy of the frequency band of a signal using FFT.\"\"\"\n","    fft_values = np.fft.fft(signal)\n","    magnitude_squared = np.abs(fft_values) ** 2\n","    total_energy = np.sum(magnitude_squared) / len(signal)\n","    return total_energy\n","\n","def max_power(signal):\n","    \"\"\"Calculates the maximum power of a signal using FFT.\"\"\"\n","    fft_values = np.fft.fft(signal)\n","    power_spectrum = np.abs(fft_values) ** 2 / len(signal)\n","    return np.max(power_spectrum)\n","\n","def shannon_entropy(signal):\n","    \"\"\"Calculates Shannon entropy of a signal.\"\"\"\n","    value, counts = np.unique(signal, return_counts=True)\n","    probabilities = counts / counts.sum()\n","    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # To avoid log(0)\n","    return entropy\n","\n","def calculate_peak_to_peak(signal):\n","    \"\"\"Calculates the peak-to-peak value of a signal.\"\"\"\n","    peak_to_peak_value = signal.max() - signal.min()\n","    return peak_to_peak_value\n","\n","def extract_features(data, num_windows):\n","    window_size = int((60*256) / num_windows)  # Adjusted to dataset specifics\n","    features, labels = [], []\n","    for i in range(num_windows):\n","        window_data = data.iloc[i*window_size : (i+1)*window_size]\n","        if len(window_data) < window_size:\n","            raise ValueError(\"NOT ENOUGH DATA FOR 1 MINUTE.\")\n","        \n","        # Drop label from the window data if present\n","        if 'label' in window_data.columns:\n","            labels.append(window_data['label'].iloc[0])\n","            feature_data = window_data.drop(columns='label')\n","        else:\n","            feature_data = window_data\n","\n","        # Calculate features for each channel\n","        means = feature_data.mean()\n","        max_powers = feature_data.apply(max_power)\n","        entropies = feature_data.apply(shannon_entropy)\n","        band_energies = feature_data.apply(calculate_total_band_energy)\n","        peak_to_peak_values = feature_data.apply(calculate_peak_to_peak)\n","\n","        # Combine all features into a single array\n","        combined_features = np.concatenate([\n","            means.values,\n","            max_powers.values,\n","            entropies.values,\n","            band_energies.values,\n","            peak_to_peak_values.values\n","        ])\n","\n","        # Append combined features and labels\n","        features.append(combined_features)\n","\n","    features, labels = np.array(features), np.array(labels)\n","    return features, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import gaussian_kde\n","from scipy.integrate import quad\n","\n","def differential_entropy(signal):\n","    \"\"\"Calcula la entropía diferencial de una señal utilizando la estimación de densidad kernel.\"\"\"\n","    kde = gaussian_kde(signal)\n","    # Definimos los límites para la integración basados en los datos\n","    lower_limit, upper_limit = signal.min(), signal.max()\n","    # Calculamos la entropía diferencial\n","    entropy = -quad(lambda x: kde(x) * np.log(kde(x) + 1e-10), lower_limit, upper_limit)[0]\n","    return entropy\n","\n","def extract_features(data, num_windows):\n","    window_size = int((60*256) / num_windows)  # Ajustado a las especificaciones del conjunto de datos\n","    features, labels = [], []\n","    for i in range(num_windows):\n","        window_data = data.iloc[i*window_size : (i+1)*window_size]\n","        if len(window_data) < window_size:\n","            raise ValueError(\"NOT ENOUGH DATA FOR 1 MINUTE.\")\n","        \n","        # Drop label from the window data if present\n","        if 'label' in window_data.columns:\n","            labels.append(window_data['label'].iloc[0])\n","            feature_data = window_data.drop(columns='label')\n","        else:\n","            feature_data = window_data\n","        \n","        # Calculate differential entropy for each channel\n","        diff_entropies = feature_data.apply(differential_entropy)\n","\n","        # Store the calculated differential entropies\n","        features.append(diff_entropies.values)\n","\n","    features, labels = np.array(features), np.array(labels)\n","    return features, labels\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOJnYtHBIv8Enmhf4iTycLN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
