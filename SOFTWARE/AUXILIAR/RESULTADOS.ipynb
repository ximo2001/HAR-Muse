{"cells":[{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import csv\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n","from scipy.stats import gmean"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["folder_path = '/home/ximo/Escritorio/ProyectoTFG/featuresExtended'\n","output_path = '/home/ximo/Escritorio/ProyectoTFG/results'\n","fichero_con_resultados = '/home/ximo/Escritorio/ProyectoTFG/results/RESULTS.csv'\n","report_file_path = '/home/ximo/Escritorio/ProyectoTFG/results/RESULTS.txt'\n","\n","def calc_metrics(cm, labels):\n","    \"\"\"Calcula las métricas basadas en la matriz de confusión para cada clase y en general.\"\"\"\n","    # Verifica que la matriz de confusión sea cuadrada\n","    assert cm.shape[0] == cm.shape[1], \"La matriz de confusión debe ser cuadrada.\"\n","    \n","    # Accuracy general\n","    acc = np.trace(cm) / np.sum(cm)  \n","    \n","    # Sensibilidad (recall) por clase\n","    sens = cm.diagonal() / cm.sum(axis=1)  \n","    \n","    # Predicciones y etiquetas verdaderas concatenadas\n","    true_labels = np.concatenate([np.full(fill_value=label, shape=(int(cm[i, :].sum()),), dtype=int) for i, label in enumerate(labels)])\n","    pred_labels = np.concatenate([np.full(fill_value=label, shape=(int(cm[:, j].sum()),), dtype=int) for j, label in enumerate(labels)])\n","        \n","    # F1-Score por clase\n","    f1_scores = f1_score(true_labels, pred_labels, average=None)\n","    \n","    # G-Mean general (geometric mean de las sensibilidades)\n","    gmean_value = gmean(sens)\n","    \n","    # Cálculo correcto de especificidad por clase\n","    spec = []\n","    for i in range(len(labels)):\n","        tn = np.sum(cm) - (np.sum(cm[:, i]) + np.sum(cm[i, :]) - cm[i, i])\n","        fp = np.sum(cm[:, i]) - cm[i, i]\n","        spec.append(tn / (tn + fp))\n","    spec = np.array(spec)\n","    \n","    # Sensibilidad general (promedio de las sensibilidades por clase)\n","    sens_general = np.mean(sens)\n","    \n","    # Especificidad general (promedio de las especificidades por clase)\n","    spec_general = np.mean(spec)\n","    \n","    # F1-Score general (promedio de los F1-Scores por clase)\n","    f1_general = np.mean(f1_scores)\n","    \n","    return {\n","        \"Accuracy General\": acc,\n","        \"G-Mean General\": gmean_value,\n","        \"F1-Score General\": f1_general,\n","        \"Sensibilidad Promedio\": sens_general,\n","        \"Especificidad General\": spec_general,\n","        \"F1-Scores por Clase\": f1_scores,\n","        \"Sensibilidades por Clase\": sens,\n","        \"Especificidades por Clase\": spec\n","    }"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["\n","def summarize_results(output_path):\n","    results = []\n","    activity_labels = {0: 'Música', 1: 'Leer', 2: 'Meditar', 3: 'Matemáticas'}\n","    columns = ['Model', 'Window', 'K', 'Accuracy', 'G-Mean', 'F1-Score General', 'Sensibilidad Promedio', 'Especificidad General']\n","\n","    # Leer el archivo de tiempos de modelo\n","    time_df_path = os.path.join(output_path, \"model_times.csv\")\n","    if os.path.exists(time_df_path):\n","        time_df = pd.read_csv(time_df_path)\n","        time_df['Window'] = time_df['Window'].astype(str)\n","        time_df['K'] = time_df['K'].astype(str)\n","    else:\n","        print(\"Archivo de tiempos no encontrado.\")\n","        return pd.DataFrame(columns=columns)  # Devolver DataFrame vacío si no se encuentra el archivo\n","\n","    first_file = True\n","    for file_name in os.listdir(output_path):\n","        if file_name.startswith(\"conf_matrix_\"):\n","            parts = file_name.split('_')\n","            window = parts[2]\n","            model = parts[3]\n","            k = parts[4].replace('.csv', '')\n","\n","            cm_df = pd.read_csv(os.path.join(output_path, file_name), index_col=0)\n","            cm = cm_df.values\n","            labels = [int(label) for label in cm_df.index]\n","\n","            # Configurar columnas dinámicas en la primera iteración\n","            if first_file:\n","                for label in labels:\n","                    label_str = activity_labels[label]\n","                    columns.extend([f'F1-Score_{label_str}', f'Sensitivity_{label_str}', f'Specificity_{label_str}'])\n","                columns.append('Time(s)')  # Agregar la columna de tiempo promedio al final\n","                first_file = False\n","\n","            metrics = calc_metrics(cm, labels)\n","            row = [\n","                model, window, k, metrics[\"Accuracy General\"], \n","                metrics[\"G-Mean General\"], metrics[\"F1-Score General\"], metrics[\"Sensibilidad Promedio\"], metrics[\"Especificidad General\"]\n","            ]\n","            for f1, s, sp in zip(metrics[\"F1-Scores por Clase\"], metrics[\"Sensibilidades por Clase\"], metrics[\"Especificidades por Clase\"]):\n","                row.extend([f1, s, sp])\n","\n","            # Obtener el tiempo de ejecución del modelo\n","            time_row = time_df[(time_df['Model'] == model) & (time_df['Window'] == window) & (time_df['K'] == k)]\n","            if not time_row.empty:\n","                model_time = time_row['Time(s)'].iloc[0]\n","            else:\n","                model_time = 'N/A'\n","            row.append(model_time)\n","\n","            results.append(row)\n","\n","    results_df = pd.DataFrame(results, columns=columns)\n","    return results_df\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)\n","results_df.to_csv(fichero_con_resultados, index=False)  # Ajusta esta ruta según sea necesario\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Report has been saved to /home/ximo/Escritorio/ProyectoTFG/results/RESULTS.txt\n"]}],"source":["import os\n","import pandas as pd\n","\n","def load_results(folder_path):\n","    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","    df_list = []\n","    for file in files:\n","        df = pd.read_csv(os.path.join(folder_path, file))\n","        df_list.append(df)\n","    results_df = pd.concat(df_list, ignore_index=True)\n","    drop_cols = [col for col in results_df.columns if any(char.isdigit() for char in col)]\n","    results_df.drop(columns=drop_cols, inplace=True)\n","    return results_df\n","\n","def find_best_metrics(results_df):\n","    max_accuracy = results_df['Accuracy'].max()\n","    max_bal_acc = results_df['Sensibilidad Promedio'].max()\n","    max_g_mean = results_df['G-Mean'].max()\n","\n","    best_accuracy = results_df[results_df['Accuracy'] == max_accuracy]\n","    best_bal_acc = results_df[results_df['Sensibilidad Promedio'] == max_bal_acc]\n","    best_g_mean = results_df[results_df['G-Mean'] == max_g_mean]\n","\n","    return best_accuracy, best_bal_acc, best_g_mean\n","\n","def format_confusion_matrix(folder_path, model, window, k):\n","    k = int(k)\n","    window = int(window)\n","    filename = f\"conf_matrix_{window}_{model}_{k}.csv\"\n","    file_path = os.path.join(folder_path, filename)\n","    cm_df = pd.read_csv(file_path, index_col=0)\n","    activity_labels = {0: 'Música', 1: 'Leer', 2: 'Meditar', 3: 'Matemáticas'}\n","    cm_df.columns = cm_df.columns.astype(int)\n","    cm_df.index = cm_df.index.map(activity_labels)\n","    cm_df.columns = cm_df.columns.map(activity_labels)\n","    cm_normalized = cm_df.div(cm_df.sum(axis=1), axis=0)\n","    return cm_normalized.round(3).to_string()\n","\n","def create_report_file(folder_path, results_df, output_file_path):\n","    best_accuracy, best_bal_acc, best_g_mean = find_best_metrics(results_df)\n","    \n","    with open(output_file_path, 'w') as file:\n","        file.write(\"Best Accuracy Models Info:\\n\")\n","        file.write(best_accuracy.to_string())\n","        file.write(\"\\n\\nMatrices de Confusión Normalizadas por Filas (Accuracy):\\n\")\n","        for _, row in best_accuracy.iterrows():\n","            file.write(f\"Model: {row['Model']} - Window: {row['Window']} - K: {row['K']}\\n\")\n","            file.write(format_confusion_matrix(folder_path, row['Model'], row['Window'], row['K']))\n","            file.write(\"\\n\")\n","\n","        file.write(\"\\nBest mean Recall Models Info:\\n\")\n","        file.write(best_bal_acc.to_string())\n","        file.write(\"\\n\\nMatrices de Confusión Normalizadas por Filas (mean recall`):\\n\")\n","        for _, row in best_bal_acc.iterrows():\n","            file.write(f\"Model: {row['Model']} - Window: {row['Window']} - K: {row['K']}\\n\")\n","            file.write(format_confusion_matrix(folder_path, row['Model'], row['Window'], row['K']))\n","            file.write(\"\\n\")\n","\n","        file.write(\"\\nBest G-Mean Models Info:\\n\")\n","        file.write(best_g_mean.to_string())\n","        file.write(\"\\n\\nMatrices de Confusión Normalizadas por Filas (G-Mean):\\n\")\n","        for _, row in best_g_mean.iterrows():\n","            file.write(f\"Model: {row['Model']} - Window: {row['Window']} - K: {row['K']}\\n\")\n","            file.write(format_confusion_matrix(folder_path, row['Model'], row['Window'], row['K']))\n","            file.write(\"\\n\")\n","\n","    print(f'Report has been saved to {output_file_path}')\n","\n","results_df = load_results(output_path)\n","create_report_file(output_path, results_df, report_file_path)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Report has been saved to /home/ximo/Escritorio/ProyectoTFG/results/RESULTSSTACKING.txt\n"]}],"source":["def load_results(folder_path):\n","    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","    df_list = []\n","    for file in files:\n","        df = pd.read_csv(os.path.join(folder_path, file))\n","        df_list.append(df)\n","    results_df = pd.concat(df_list, ignore_index=True)\n","    drop_cols = [col for col in results_df.columns if any(char.isdigit() for char in col)]\n","    results_df.drop(columns=drop_cols, inplace=True)\n","    # Asegurar que NaN en la columna 'Model' no cause problemas\n","    results_df = results_df[results_df['Model'].str.contains('StackingClassifier', na=False)]\n","    return results_df\n","\n","def find_best_by_metric(results_df, metric):\n","    results_grouped = results_df.groupby('Window')\n","    best_models = {}\n","    for window, group in results_grouped:\n","        max_value = group[metric].max()\n","        best_models[window] = group[group[metric] == max_value]\n","    return best_models\n","\n","def create_stacking_report(folder_path, output_file_path):\n","    results_df = load_results(folder_path)\n","    \n","    best_by_accuracy = find_best_by_metric(results_df, 'Accuracy')\n","    best_by_bal_acc = find_best_by_metric(results_df, 'Sensibilidad Promedio')\n","    best_by_g_mean = find_best_by_metric(results_df, 'G-Mean')\n","\n","    with open(output_file_path, 'w') as file:\n","        file.write(\"StackingClassifier Performance Report\\n\\n\")\n","        \n","        windows = results_df['Window'].unique()\n","        for window in sorted(windows):\n","            file.write(f\"Window: {window}\\n\")\n","            if window in best_by_accuracy:\n","                file.write(\"Best Accuracy:\\n\")\n","                file.write(best_by_accuracy[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","            if window in best_by_bal_acc:\n","                file.write(\"Best mean Recall:\\n\")\n","                file.write(best_by_bal_acc[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","            if window in best_by_g_mean:\n","                file.write(\"Best G-Mean:\\n\")\n","                file.write(best_by_g_mean[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","    \n","    print(f'Report has been saved to {output_file_path}')\n","\n","# Asegúrate de definir 'output_path' correctamente\n","report_file_path = '/home/ximo/Escritorio/ProyectoTFG/results/RESULTSSTACKING.txt'\n","create_stacking_report(output_path, report_file_path)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Report has been saved to /home/ximo/Escritorio/ProyectoTFG/results/RESULTSVOTING.txt\n"]}],"source":["def load_results(folder_path):\n","    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","    df_list = []\n","    for file in files:\n","        df = pd.read_csv(os.path.join(folder_path, file))\n","        df_list.append(df)\n","    results_df = pd.concat(df_list, ignore_index=True)\n","    drop_cols = [col for col in results_df.columns if any(char.isdigit() for char in col)]\n","    results_df.drop(columns=drop_cols, inplace=True)\n","    # Asegurar que NaN en la columna 'Model' no cause problemas\n","    results_df = results_df[results_df['Model'].str.contains('VotingClassifier', na=False)]\n","    return results_df\n","\n","def find_best_by_metric(results_df, metric):\n","    results_grouped = results_df.groupby('Window')\n","    best_models = {}\n","    for window, group in results_grouped:\n","        max_value = group[metric].max()\n","        best_models[window] = group[group[metric] == max_value]\n","    return best_models\n","\n","def create_stacking_report(folder_path, output_file_path):\n","    results_df = load_results(folder_path)\n","    \n","    best_by_accuracy = find_best_by_metric(results_df, 'Accuracy')\n","    best_by_bal_acc = find_best_by_metric(results_df, 'Sensibilidad Promedio')\n","    best_by_g_mean = find_best_by_metric(results_df, 'G-Mean')\n","\n","    with open(output_file_path, 'w') as file:\n","        file.write(\"VotingClassifier Performance Report\\n\\n\")\n","        \n","        windows = results_df['Window'].unique()\n","        for window in sorted(windows):\n","            file.write(f\"Window: {window}\\n\")\n","            if window in best_by_accuracy:\n","                file.write(\"Best Accuracy:\\n\")\n","                file.write(best_by_accuracy[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","            if window in best_by_bal_acc:\n","                file.write(\"Best mean Recall:\\n\")\n","                file.write(best_by_bal_acc[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","            if window in best_by_g_mean:\n","                file.write(\"Best G-Mean:\\n\")\n","                file.write(best_by_g_mean[window].to_string(index=False))\n","                file.write(\"\\n\\n\")\n","    \n","    print(f'Report has been saved to {output_file_path}')\n","\n","# Asegúrate de definir 'output_path' correctamente\n","report_file_path = '/home/ximo/Escritorio/ProyectoTFG/results/RESULTSVOTING.txt'\n","create_stacking_report(output_path, report_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS'\n","fichero_con_resultados = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTS.csv'\n","report_file_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTS.txt'\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)\n","results_df.to_csv(fichero_con_resultados, index=False)  # Ajusta esta ruta según sea necesario"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Resultados guardados en RESULTSmedios.txt\n"]}],"source":["import pandas as pd\n","\n","# Cargar el archivo CSV\n","file_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTS.csv'\n","df = pd.read_csv(file_path)\n","\n","# Calcular la media de Accuracy, Sensibilidad Promedio y Specificidad General por modelo\n","grouped_df = df.groupby('Model').agg({\n","    'Accuracy': 'mean',\n","    'Sensibilidad Promedio': 'mean',\n","    'Especificidad General': 'mean'\n","}).reset_index()\n","\n","# Ordenar los modelos por Accuracy medio en orden descendente\n","sorted_df = grouped_df.sort_values(by='Accuracy', ascending=False)\n","\n","# Guardar los resultados en un archivo de texto\n","output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSmedios.txt'\n","with open(output_path, 'w') as f:\n","    f.write(f\"{'Model':<15}{'Accuracy Medio':<15}{'Sensibilidad Promedio Media':<30}{'Specificidad General Media':<30}\\n\")\n","    f.write(\"=\"*90 + \"\\n\")\n","    for _, row in sorted_df.iterrows():\n","        f.write(f\"{row['Model']:<15}{row['Accuracy']:<15.4f}{row['Sensibilidad Promedio']:<30.4f}{row['Especificidad General']:<30.4f}\\n\")\n","\n","print(\"Resultados guardados en RESULTSmedios.txt\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS'\n","result_file_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTS.csv'\n","report_file_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/windows.txt'\n","\n","def create_window_ranking(output_path, result_file):\n","    df = pd.read_csv(result_file)\n","    # Filter only the required models\n","    df = df[df['Model'].isin(['StackingClassifier', 'RandomForest', 'CatBoost', 'VotingClassifier'])]\n","    metrics = ['Accuracy', 'Sensibilidad Promedio', 'Especificidad General']\n","\n","    results = {}\n","    for metric in metrics:\n","        # Calculate the sum of each metric per window and divide by the number of entries to get the average\n","        grouped = df.groupby('Window')[metric].agg(['sum', 'count'])\n","        grouped['average'] = grouped['sum'] / grouped['count']\n","        results[metric] = grouped['average'].sort_values(ascending=False)\n","\n","    with open(report_file_path, 'w') as file:\n","        for metric, data in results.items():\n","            file.write(f\"Best window for {metric}:\\n\")\n","            for window, score in data.items():\n","                # Change here to ensure all decimals are shown\n","                file.write(f\"{window} {score}\\n\")\n","            file.write(\"\\n\")\n","\n","# Execute the function to process and save the rankings\n","create_window_ranking(output_path, result_file_path)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Los archivos ordenados han sido guardados.\n"]}],"source":["def order_results_by_model_and_window(results_df):\n","    # Ordenar primero por modelo de forma alfabética y luego por ventana\n","    ordered_df = results_df.sort_values(by=['Model', 'Window'], ascending=[True, True])\n","    return ordered_df\n","\n","def order_results_by_window_and_model(results_df):\n","    # Convertir Window a entero para un orden numérico correcto, si es necesario\n","    results_df['Window'] = results_df['Window'].astype(int)\n","    # Ordenar primero por ventana de forma ascendente y luego por modelo de forma alfabética\n","    ordered_df = results_df.sort_values(by=['Window', 'Model'], ascending=[True, True])\n","    return ordered_df\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)  # Asegúrate de que esta función se haya llamado correctamente anteriormente\n","\n","# Ordenar y guardar los archivos\n","results_ordered_by_model = order_results_by_model_and_window(results_df)\n","results_ordered_by_window = order_results_by_window_and_model(results_df)\n","\n","# Guardar los archivos CSV ordenados\n","results_ordered_by_model.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDMODEL.csv', index=False)\n","results_ordered_by_window.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDWINDOW.csv', index=False)\n","\n","print(\"Los archivos ordenados han sido guardados.\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Los archivos ordenados por métricas han sido guardados.\n"]}],"source":["def order_results_by_metric(results_df, metric):\n","    # Ordenar por la métrica especificada de forma descendente\n","    ordered_df = results_df.sort_values(by=[metric], ascending=False)\n","    return ordered_df\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)  \n","\n","# Ordenar y guardar los archivos por cada métrica\n","results_ordered_by_accuracy = order_results_by_metric(results_df, 'Accuracy')\n","results_ordered_by_bal_acc = order_results_by_metric(results_df, 'Sensibilidad Promedio')\n","results_ordered_by_g_mean = order_results_by_metric(results_df, 'G-Mean')\n","results_ordered_by_specifity = order_results_by_metric(results_df, 'Especificidad General')\n","\n","\n","# Guardar los archivos CSV ordenados\n","results_ordered_by_accuracy.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDACCURACY.csv', index=False)\n","results_ordered_by_bal_acc.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDBALANCEDACCURACY.csv', index=False)\n","results_ordered_by_g_mean.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDGMEAN.csv', index=False)\n","results_ordered_by_specifity.to_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDESPECIFITY.csv', index=False)\n","\n","print(\"Los archivos ordenados por métricas han sido guardados.\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["El análisis de ranking ha sido guardado en analisis.txt.\n"]}],"source":["import pandas as pd\n","\n","# Cargar los ficheros ordenados\n","accuracy_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDACCURACY.csv')\n","balanced_acc_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDBALANCEDACCURACY.csv')\n","g_mean_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDESPECIFITY.csv')\n","\n","def calculate_ranking(*dataframes):\n","    model_scores = {}\n","\n","    # Calcular puntuaciones para cada métrica\n","    for df in dataframes:\n","        for i, row in enumerate(df.itertuples(), start=1):\n","            if row.Model in model_scores:\n","                model_scores[row.Model].append(i)\n","            else:\n","                model_scores[row.Model] = [i]\n","    \n","    # Normalizar puntuaciones dividiendo por el número de apariciones\n","    for model, scores in model_scores.items():\n","        model_scores[model] = sum(scores) / len(scores)\n","\n","    # Ordenar los modelos por su puntuación promedio\n","    ranked_models = sorted(model_scores.items(), key=lambda item: item[1])\n","\n","    return ranked_models\n","\n","# Calcular rankings\n","accuracy_ranking = calculate_ranking(accuracy_df)\n","balanced_accuracy_ranking = calculate_ranking(balanced_acc_df)\n","g_mean_ranking = calculate_ranking(g_mean_df)\n","\n","# Escribir los resultados en un archivo de texto\n","with open('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/analisis.txt', 'w') as file:\n","    file.write(\"Ordenados por Accuracy:\\n\")\n","    for model, score in accuracy_ranking:\n","        file.write(f\"{model}: {score:.2f}\\n\")\n","    file.write(\"\\nOrdenados por mean Recall:\\n\")\n","    for model, score in balanced_accuracy_ranking:\n","        file.write(f\"{model}: {score:.2f}\\n\")\n","    file.write(\"\\nOrdenados por Specifity:\\n\")\n","    for model, score in g_mean_ranking:\n","        file.write(f\"{model}: {score:.2f}\\n\")\n","\n","print(\"El análisis de ranking ha sido guardado en analisis.txt.\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Los rankings han sido compilados y guardados en 'ranking.txt'.\n"]}],"source":["import pandas as pd\n","\n","def calculate_weighted_ranking(df, metric):\n","    # Asegurar que el DataFrame está ordenado por la métrica\n","    df = df.sort_values(by=[metric], ascending=False)\n","    # Agregar una columna de ranking basado en la posición\n","    df['Rank'] = df[metric].rank(method='average', ascending=False)\n","    # Agrupar por modelo y calcular el promedio de los rangos\n","    df_grouped = df.groupby('Model')['Rank'].mean().reset_index()\n","    df_grouped = df_grouped.sort_values('Rank')\n","    return df_grouped\n","\n","def compile_all_rankings(accuracy_df, balanced_acc_df, g_mean_df):\n","    # Calcular los rankings ponderados para cada métrica\n","    accuracy_ranking = calculate_weighted_ranking(accuracy_df, 'Accuracy')\n","    balanced_accuracy_ranking = calculate_weighted_ranking(balanced_acc_df, 'Sensibilidad Promedio')\n","    g_mean_ranking = calculate_weighted_ranking(g_mean_df, 'Especificidad General')\n","    \n","    # Preparar un DataFrame general para el ranking combinado\n","    combined = accuracy_ranking.rename(columns={'Rank': 'Rank_Accuracy'})\n","    combined = combined.merge(balanced_accuracy_ranking.rename(columns={'Rank': 'Rank_Balanced_Accuracy'}), on='Model', how='outer')\n","    combined = combined.merge(g_mean_ranking.rename(columns={'Rank': 'Rank_Especifidad'}), on='Model', how='outer')\n","    \n","    # Calcular el ranking promedio\n","    combined['Average_Rank'] = combined[['Rank_Accuracy', 'Rank_Balanced_Accuracy', 'Rank_Especifidad']].mean(axis=1)\n","    combined = combined.sort_values('Average_Rank')\n","\n","    # Escribir todos los rankings en un solo archivo\n","    with open('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/ranking.txt', 'w') as f:\n","        f.write(\"Ranking by Accuracy:\\n\")\n","        f.write(accuracy_ranking.to_string(index=False) + \"\\n\\n\")\n","        f.write(\"Ranking by mean Recall:\\n\")\n","        f.write(balanced_accuracy_ranking.to_string(index=False) + \"\\n\\n\")\n","        f.write(\"Ranking by Especifidad:\\n\")\n","        f.write(g_mean_ranking.to_string(index=False) + \"\\n\\n\")\n","        f.write(\"General Ranking Across Metrics:\\n\")\n","        f.write(combined[['Model', 'Average_Rank']].to_string(index=False))\n","\n","    print(\"Los rankings han sido compilados y guardados en 'ranking.txt'.\")\n","\n","# Cargar los datos\n","accuracy_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDACCURACY.csv')\n","balanced_acc_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDBALANCEDACCURACY.csv')\n","g_mean_df = pd.read_csv('/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTSORDEREDESPECIFITY.csv')\n","\n","# Compilar los rankings y generar el archivo\n","compile_all_rankings(accuracy_df, balanced_acc_df, g_mean_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 3 rows with the highest accuracy for window 4:\n","                 Model  Window   K  Accuracy    G-Mean  F1-Score General  \\\n","15            CatBoost       4  42  0.566667  0.558823          0.905413   \n","5     VotingClassifier       4  50  0.559375  0.548262          0.797867   \n","7   StackingClassifier       4  45  0.556250  0.546879          0.974050   \n","\n","    Sensibilidad Promedio  Especificidad General  F1-Score_Música  \\\n","15               0.566667               0.855556         0.958722   \n","5                0.559375               0.853125         0.879658   \n","7                0.556250               0.852083         0.989691   \n","\n","    Sensitivity_Música  ...  F1-Score_Leer  Sensitivity_Leer  \\\n","15            0.566667  ...       0.882629          0.559722   \n","5             0.630556  ...       0.716438          0.533333   \n","7             0.540278  ...       0.975818          0.498611   \n","\n","    Specificity_Leer  F1-Score_Meditar  Sensitivity_Meditar  \\\n","15          0.829630          0.852528             0.702778   \n","5           0.835185          0.724784             0.676389   \n","7           0.848611          0.960000             0.733333   \n","\n","    Specificity_Meditar  F1-Score_Matemáticas  Sensitivity_Matemáticas  \\\n","15             0.908333              0.927774                 0.437500   \n","5              0.916204              0.870588                 0.397222   \n","7              0.883333              0.970693                 0.452778   \n","\n","    Specificity_Matemáticas      Time(s)  \n","15                 0.857407    28.342659  \n","5                  0.875463   137.510888  \n","7                  0.836574  4074.404243  \n","\n","[3 rows x 21 columns]\n"]}],"source":["import pandas as pd\n","import os\n","\n","# Paths\n","result_file_path = '/home/ximo/Escritorio/ProyectoTFG/resultsIGUALADOS/RESULTS.csv'\n","\n","def print_top_accuracy_rows(result_file, window, top_n=3):\n","    # Load data from the results file\n","    df = pd.read_csv(result_file)\n","    \n","    # Ensure that 'Window' is interpreted as an integer, if not already\n","    df['Window'] = df['Window'].astype(int)\n","\n","    # Filter for the specified window\n","    window_df = df[df['Window'] == window]\n","    \n","    # Check if there are any entries for this window\n","    if not window_df.empty:\n","        # Sort by 'Accuracy' and get the top N rows, considering ties\n","        top_accuracy_rows = window_df.nlargest(top_n, 'Accuracy')\n","        print(f\"Top {top_n} rows with the highest accuracy for window {window}:\")\n","        print(top_accuracy_rows)\n","    else:\n","        print(f\"No data found for window {window}.\")\n","\n","# Specify the window you are interested in\n","window_of_interest = 4\n","print_top_accuracy_rows(result_file_path, window_of_interest)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsEXTRA'\n","fichero_con_resultados = '/home/ximo/Escritorio/ProyectoTFG/resultsEXTRA/RESULTS.csv'\n","\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)\n","results_df.to_csv(fichero_con_resultados, index=False)  # Ajusta esta ruta según sea necesario"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsBINMATES'\n","fichero_con_resultados = '/home/ximo/Escritorio/ProyectoTFG/resultsBINMATES/RESULTS.csv'\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)\n","results_df.to_csv(fichero_con_resultados, index=False)  # Ajusta esta ruta según sea necesario"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["output_path = '/home/ximo/Escritorio/ProyectoTFG/resultsBINMUSICA'\n","fichero_con_resultados = '/home/ximo/Escritorio/ProyectoTFG/resultsBINMUSICA/RESULTS.csv'\n","\n","# Llamar a la función y guardar los resultados\n","results_df = summarize_results(output_path)\n","results_df.to_csv(fichero_con_resultados, index=False)  # Ajusta esta ruta según sea necesario"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOJnYtHBIv8Enmhf4iTycLN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
